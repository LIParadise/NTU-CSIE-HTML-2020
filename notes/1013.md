breakpoints are not focus of this course, Coursera week 6
Just imagine that we have breakpoint and if they are found the $m_H(n)$ would grow slow.

non-breakpoint
dc-dimension
vc-dimension for PLA
linear algebra would be appreciated

Actually, the bound isn't directly putting $m_H(n)$ in Hoeffding's eqn... We're not so lucky: bad things happen, and we need to loose our bounds. However, these constants are not easy to derive, rather they are quite technical and tricky. Unfortunately, vc bound proof would be omitted in this course. (Both are Russian people) (失眠良伴) (Also they don't focus on binary only: regression, multi-class, etc, so the sketch isn't so similar; however the textbook have binary case proof that's easier to understand.)
General idea is that if breakpoints exists, it's slow, and to be specific it shall be of polynomial up to (n-1) times when break happens at (n) -- vc dimension is just (n-1) if first break is (n).

Break implies it's larger than vc dimension; if so, 2^n is impossible to achieve; impossible to shatter.
No larger than vc dimension --> maybe there exists a set of data points that could be shattered.

Theory of Generalization: 舉一反三
確認 Ein 跟 Eout 有點類似
如果資料夠多 & 有露出曙光 (break)

Actually B(n,k) << n^(k-1) for n \geq 2, k \geq 3

VC bound: bad hypothesis happen probability is bounded with exponential and m_H(n)
Vapnik Chervonenkis bound

Good hypothesis set: breaks.
Good data: enough data
舉一反三!!!
Now all that left is a good algorithm to choose low E_\text{in}
Or do I...?
Nope, we still need luck lol

# VC Dimension

*The largest natural number that's not breakpoint*
The most inputs that our hypothesis set \mathcal{H} can *shatter*; to be more specific, there exists some data points of (vc dimension) size and exists some subset of hypothesis that can shatter them.
Good hypothesis set: finite vc dimension; Guarantee E_\text{out} is near E_\text{in}

## <=

we need to find for every subset of data points a dichotomy that don't exists
linear dependent, so we could build a dichotomy there's no way you can achieve.

__*等效二元分類的自由度*__
__*VC Dimension*__
approximate vc dimension: nub number

## Useless of VC dimension?

DL; in these VCD are rediculously large, too large that they shall probably met bad situation

we have other complexity measure on hypothesis set, besides VCD

Anyway at least we now have a measure!

## New DL Progress

E_in continue to fall; E_out may have second fall!
The latter is ongoing research topic.

# Q

不用假設P 的形狀，仍能得到learning 的保證
跟P沒關係
Distribution-Free
假設P 的形狀，或許我們可以得到更強的結論
但是目前在這邊我們不需要這個，就能得到這樣的好結果( E_in ~= E_out )
- Hoeffding; iid is enough
- We already considered the worst situation by define m_H(n) with \max

Multi-Class?
in VC setting, they first use binary case, then consider multi class.

抽?

# Back to $P$ of the learning scheme

$P$ is the distribution governing how we sample in the universal data, both for how we have (data on hand), and how we would interpret $E_\text{out}$.

E_\text{out}?
Yes, E_\text{out}.
The coloring is based on the hypothesis and the oracle, which are, in current settings, deterministic.
E_\text{out} is this ratio of (correct hypothesis)/(false hypothesis) as seen by the $P$.
So what Hoeffding have to do with the learning scheme is that given a jar (a deterministic, known result based on the given hypothesis corresponding to the jar) and *distribution for __sampling__ data $P$*, we would have the story of (E_in close to E_out) being very likely.
Recall that the data on hand are assumed to be correct, that is, given by oracle $f$. So we can calculate E_in. And by Hoeffding, we know it's highly possible that it's closed to E_out. Nice.
